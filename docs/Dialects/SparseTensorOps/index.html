<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><title>'sparse_tensor' Dialect - MLIR</title><meta name=description content="Multi-Level IR Compiler Framework"><meta name=generator content="Hugo 0.80.0"><link href=https://mlir.llvm.org/index.xml rel=alternate type=application/rss+xml><link rel=canonical href=https://mlir.llvm.org/docs/Dialects/SparseTensorOps/><link rel=stylesheet href=https://mlir.llvm.org/css/theme.css><script src=https://use.fontawesome.com/releases/v5.0.6/js/all.js></script><link rel=stylesheet href=https://mlir.llvm.org/css/chroma.min.css><script src=https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js></script><script src=https://cdn.jsdelivr.net/npm/jquery.easing@1.4.1/jquery.easing.min.js></script><script src=https://mlir.llvm.org/js/bundle.js></script><script type=text/javascript src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type=text/x-mathjax-config>
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$', '$'] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
    }
  });
</script><link rel=apple-touch-icon sizes=180x180 href="/apple-touch-icon.png?v=1"><link rel=icon type=image/png sizes=32x32 href="/favicon-32x32.png?v=1"><link rel=icon type=image/png sizes=16x16 href="/favicon-16x16.png?v=1"><link rel=manifest href="/site.webmanifest?v=1"><link rel=mask-icon href="/safari-pinned-tab.svg?v=1" color=#3775e0><link rel="shortcut icon" href="/favicon.ico?v=1"><meta name=msapplication-TileColor content="#2d89ef"><meta name=theme-color content="#ffffff"><link rel=icon href=/favicon.svg type=image/svg+xml sizes=any><style>:root{}</style></head><body><div class=container><header><h1><div><img src=https://mlir.llvm.org//mlir-logo.png width=40px align=absmiddle>
MLIR</div></h1><p class=description>Multi-Level IR Compiler Framework</p></header><div class=global-menu><nav><ul><li class=parent><a href>Community<i class="fas fa-angle-right"></i></a><ul class=sub-menu><li class=child><a href=https://llvm.discourse.group/c/mlir/31>Forums</a></li><li class=child><a href=https://discord.gg/xS7Z362>Chat</a></li></ul></li><li><a href=/getting_started/Debugging/>Debugging</a></li><li><a href=/getting_started/Faq/>FAQ</a></li><li class=parent><a href=https://github.com/llvm/llvm-project/tree/main/mlir>Source<i class="fas fa-angle-right"></i></a><ul class=sub-menu><li class=child><a href=/doxygen/>Doxygen</a></li><li class=child><a href=https://github.com/llvm/llvm-project/tree/main/mlir>GitHub</a></li></ul></li><li><a href="https://bugs.llvm.org/buglist.cgi?bug_status=__open__&list_id=177877&order=changeddate%20DESC%2Cpriority%2Cbug_severity&product=MLIR&query_format=specific">Bugs</a></li><li><a href=https://github.com/llvm/mlir-www/tree/main/website/static/LogoAssets>Logo Assets</a></li><li><a href=https://www.youtube.com/MLIRCompiler>Youtube Channel</a></li></ul></nav></div><div class=content-container><main><h1>'sparse_tensor' Dialect</h1><p>The <code>SparseTensor</code> dialect supports all the attributes, types,
operations, and passes that are required to make sparse tensor
types first class citizens within the MLIR compiler infrastructure.
The dialect forms a bridge between high-level operations on sparse
tensors types and lower-level operations on the actual sparse storage
schemes consisting of pointers, indices, and values. Lower-level
support may consist of fully generated code or may be provided by
means of a small sparse runtime support library.</p><p>The concept of <strong>treating sparsity as a property, not a tedious
implementation detail</strong>, by letting a <strong>sparse compiler</strong> generate
sparse code automatically was pioneered for dense linear algebra by
[Bik96] in MT1 (see <a href=https://www.aartbik.com/sparse.php>https://www.aartbik.com/sparse.php</a>) and formalized
to tensor algebra by [Kjolstad17,Kjolstad20] in the Sparse Tensor
Algebra Compiler (TACO) project (see <a href=http://tensor-compiler.org>http://tensor-compiler.org</a>).</p><p>The MLIR implementation closely follows the &ldquo;sparse iteration theory&rdquo;
that forms the foundation of TACO. A rewriting rule is applied to each
tensor expression in the Linalg dialect (MLIR&rsquo;s tensor index notation)
where the sparsity of tensors is indicated using the per-dimension level
types dense/compressed together with a specification of the order on the
dimensions (see [Chou18] for an in-depth discussions and possible
extensions to these level types). Subsequently, a topologically sorted
iteration graph, reflecting the required order on indices with respect
to the dimensions of each tensor, is constructed to ensure that all tensors
are visited in natural index order. Next, iteration lattices are
constructed for the tensor expression for every index in topological
order. Each iteration lattice point consists of a conjunction of tensor
indices together with a tensor (sub)expression that needs to be evaluated
for that conjunction. Within the lattice, iteration points are ordered
according to the way indices are exhausted. As such these iteration
lattices drive actual sparse code generation, which consists of a
relatively straightforward one-to-one mapping from iteration lattices
to combinations of for-loops, while-loops, and if-statements.</p><ul><li>[Bik96] Aart J.C. Bik. Compiler Support for Sparse Matrix Computations.
PhD thesis, Leiden University, May 1996.</li><li>[Kjolstad17] Fredrik Berg Kjolstad, Shoaib Ashraf Kamil, Stephen Chou, David
Lugato, and Saman Amarasinghe. The Tensor Algebra Compiler. Proceedings of
the ACM on Programming Languages, October 2017.</li><li>[Kjolstad20] Fredrik Berg Kjolstad. Sparse Tensor Algebra Compilation.
PhD thesis, MIT, February, 2020.</li><li>[Chou18] Stephen Chou, Fredrik Berg Kjolstad, and Saman Amarasinghe.
Format Abstraction for Sparse Tensor Algebra Compilers. Proceedings of
the ACM on Programming Languages, October 2018.</li></ul><p><nav id=TableOfContents><ul><li><a href=#attribute-definition>Attribute definition</a><ul><li><a href=#sparsetensorencodingattr>SparseTensorEncodingAttr</a></li></ul></li><li><a href=#operation-definition>Operation definition</a><ul><li><a href=#sparse_tensorconvert-mlirsparse_tensorconvertop><code>sparse_tensor.convert</code> (::mlir::sparse_tensor::ConvertOp)</a></li><li><a href=#sparse_tensorinit-mlirsparse_tensorinitop><code>sparse_tensor.init</code> (::mlir::sparse_tensor::InitOp)</a></li><li><a href=#sparse_tensorlex_insert-mlirsparse_tensorlexinsertop><code>sparse_tensor.lex_insert</code> (::mlir::sparse_tensor::LexInsertOp)</a></li><li><a href=#sparse_tensorload-mlirsparse_tensorloadop><code>sparse_tensor.load</code> (::mlir::sparse_tensor::LoadOp)</a></li><li><a href=#sparse_tensornew-mlirsparse_tensornewop><code>sparse_tensor.new</code> (::mlir::sparse_tensor::NewOp)</a></li><li><a href=#sparse_tensorrelease-mlirsparse_tensorreleaseop><code>sparse_tensor.release</code> (::mlir::sparse_tensor::ReleaseOp)</a></li><li><a href=#sparse_tensorindices-mlirsparse_tensortoindicesop><code>sparse_tensor.indices</code> (::mlir::sparse_tensor::ToIndicesOp)</a></li><li><a href=#sparse_tensorpointers-mlirsparse_tensortopointersop><code>sparse_tensor.pointers</code> (::mlir::sparse_tensor::ToPointersOp)</a></li><li><a href=#sparse_tensorvalues-mlirsparse_tensortovaluesop><code>sparse_tensor.values</code> (::mlir::sparse_tensor::ToValuesOp)</a></li></ul></li></ul></nav><h2 id=attribute-definition>Attribute definition&nbsp;<a class=headline-hash href=#attribute-definition>¶</a></h2><h3 id=sparsetensorencodingattr>SparseTensorEncodingAttr&nbsp;<a class=headline-hash href=#sparsetensorencodingattr>¶</a></h3><p>An attribute to encode TACO-style information on sparsity properties
of tensors. The encoding is eventually used by a <strong>sparse compiler</strong>
pass to generate sparse code fully automatically for all tensor
expressions that involve tensors with a sparse encoding. Compiler
passes that run before this sparse compiler pass need to be
aware of the semantics of tensor types with such an encoding.</p><p>Example:</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=nv>#DCSC</span> <span class=p>=</span> <span class=nv>#sparse_tensor.encoding</span><span class=p>&lt;{</span>
  <span class=nl>dimLevelType =</span> <span class=p>[</span> <span class=s>&#34;compressed&#34;</span><span class=p>,</span> <span class=s>&#34;compressed&#34;</span> <span class=p>],</span>
  <span class=nl>dimOrdering =</span> affine_map<span class=p>&lt;(</span>i<span class=p>,</span>j<span class=p>)</span> <span class=p>-&gt;</span> <span class=p>(</span>j<span class=p>,</span>i<span class=p>)&gt;,</span>
  <span class=nl>pointerBitWidth =</span> <span class=m>32</span><span class=p>,</span>
  <span class=nl>indexBitWidth =</span> <span class=m>8</span>
<span class=p>}&gt;</span>


<span class=p>...</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>8x8x</span><span class=k>f64</span><span class=p>,</span> <span class=nv>#DCSC</span><span class=p>&gt;</span> <span class=p>...</span>
</code></pre></div><h4 id=parameters>Parameters:&nbsp;<a class=headline-hash href=#parameters>¶</a></h4><table><thead><tr><th style=text-align:center>Parameter</th><th style=text-align:center>C++ type</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center>dimLevelType</td><td style=text-align:center><code>::llvm::ArrayRef&lt;SparseTensorEncodingAttr::DimLevelType></code></td><td>Per-dimension level type</td></tr><tr><td style=text-align:center>dimOrdering</td><td style=text-align:center><code>AffineMap</code></td><td></td></tr><tr><td style=text-align:center>pointerBitWidth</td><td style=text-align:center><code>unsigned</code></td><td></td></tr><tr><td style=text-align:center>indexBitWidth</td><td style=text-align:center><code>unsigned</code></td><td></td></tr></tbody></table><h2 id=operation-definition>Operation definition&nbsp;<a class=headline-hash href=#operation-definition>¶</a></h2><h3 id=sparse_tensorconvert-mlirsparse_tensorconvertop><code>sparse_tensor.convert</code> (::mlir::sparse_tensor::ConvertOp)&nbsp;<a class=headline-hash href=#sparse_tensorconvert-mlirsparse_tensorconvertop>¶</a></h3><p>Converts between different tensor types</p><p>Syntax:</p><pre><code>operation ::= `sparse_tensor.convert` $source attr-dict `:` type($source) `to` type($dest)
</code></pre><p>Converts one sparse or dense tensor type to another tensor type. The rank
of the source and destination types must match exactly, and the dimension
sizes must either match exactly or relax from a static to a dynamic size.
The sparse encoding of the two types can obviously be completely different.
The name <code>convert</code> was preferred over <code>cast</code>, since the operation may incur
a non-trivial cost.</p><p>When converting between two different sparse tensor types, only explicitly
stored values are moved from one underlying sparse storage format to
the other. When converting from an unannotated dense tensor type to a
sparse tensor type, an explicit test for nonzero values is used. When
converting to an unannotated dense tensor type, implicit zeroes in the
sparse storage format are made explicit. Note that the conversions can have
non-trivial costs associated with them, since they may involve elaborate
data structure transformations. Also, conversions from sparse tensor types
into dense tensor types may be infeasible in terms of storage requirements.</p><p>Examples:</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=nv>%0</span> <span class=p>=</span> sparse_tensor<span class=p>.</span>convert <span class=nv>%a</span> <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>32x32x</span><span class=k>f32</span><span class=p>&gt;</span> to <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>32x32x</span><span class=k>f32</span><span class=p>,</span> <span class=nv>#CSR</span><span class=p>&gt;</span>
<span class=nv>%1</span> <span class=p>=</span> sparse_tensor<span class=p>.</span>convert <span class=nv>%a</span> <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>32x32x</span><span class=k>f32</span><span class=p>&gt;</span> to <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>?x?x</span><span class=k>f32</span><span class=p>,</span> <span class=nv>#CSR</span><span class=p>&gt;</span>
<span class=nv>%2</span> <span class=p>=</span> sparse_tensor<span class=p>.</span>convert <span class=nv>%b</span> <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>8x8x</span><span class=k>i32</span><span class=p>,</span> <span class=nv>#CSC</span><span class=p>&gt;</span> to <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>8x8x</span><span class=k>i32</span><span class=p>,</span> <span class=nv>#CSR</span><span class=p>&gt;</span>
<span class=nv>%3</span> <span class=p>=</span> sparse_tensor<span class=p>.</span>convert <span class=nv>%c</span> <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>4x8x</span><span class=k>f64</span><span class=p>,</span> <span class=nv>#CSR</span><span class=p>&gt;</span> to <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>4x?x</span><span class=k>f64</span><span class=p>,</span> <span class=nv>#CSC</span><span class=p>&gt;</span>

<span class=c>// The following conversion is not allowed (since it would require a
</span><span class=c>// runtime assertion that the source&#39;s dimension size is actually 100).
</span><span class=c></span><span class=nv>%4</span> <span class=p>=</span> sparse_tensor<span class=p>.</span>convert <span class=nv>%d</span> <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>?x</span><span class=k>f64</span><span class=p>&gt;</span> to <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>100x</span><span class=k>f64</span><span class=p>,</span> <span class=nv>#SV</span><span class=p>&gt;</span>
</code></pre></div><p>Traits: SameOperandsAndResultElementType</p><p>Interfaces: NoSideEffect (MemoryEffectOpInterface)</p><p>Effects: MemoryEffects::Effect{}</p><h4 id=operands>Operands:&nbsp;<a class=headline-hash href=#operands>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>source</code></td><td>tensor of any type values</td></tr></tbody></table><h4 id=results>Results:&nbsp;<a class=headline-hash href=#results>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>dest</code></td><td>tensor of any type values</td></tr></tbody></table><h3 id=sparse_tensorinit-mlirsparse_tensorinitop><code>sparse_tensor.init</code> (::mlir::sparse_tensor::InitOp)&nbsp;<a class=headline-hash href=#sparse_tensorinit-mlirsparse_tensorinitop>¶</a></h3><p>Materializes an unitialized sparse tensor</p><p>Syntax:</p><pre><code>operation ::= `sparse_tensor.init` `[` $sizes `]` attr-dict `:` type($result)
</code></pre><p>Materializes an uninitialized sparse tensor with given shape (either static
or dynamic). The operation is provided as an anchor that materializes a
properly typed but uninitialized sparse tensor into the output clause of
a subsequent operation that yields a sparse tensor as the result.</p><p>Example:</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=nv>%c</span> <span class=p>=</span> sparse_tensor<span class=p>.</span>init_tensor <span class=p>[</span><span class=nv>%d1</span><span class=p>,</span> <span class=nv>%d2</span><span class=p>]</span> <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>?x?x</span><span class=k>f32</span><span class=p>,</span> <span class=nv>#SparseMatrix</span><span class=p>&gt;</span>
<span class=nv>%0</span> <span class=p>=</span> linalg<span class=p>.</span>matmul
  ins<span class=p>(</span><span class=nv>%a</span><span class=p>,</span> <span class=nv>%b</span><span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>?x?x</span><span class=k>f32</span><span class=p>&gt;,</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>?x?x</span><span class=k>f32</span><span class=p>&gt;)</span>
  outs<span class=p>(</span><span class=nv>%c</span><span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>?x?x</span><span class=k>f32</span><span class=p>,</span> <span class=nv>#SparseMatrix</span><span class=p>&gt;)</span> <span class=p>-&gt;</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>?x?x</span><span class=k>f32</span><span class=p>,</span> <span class=nv>#SparseMatrix</span><span class=p>&gt;</span>
</code></pre></div><p>Interfaces: NoSideEffect (MemoryEffectOpInterface)</p><p>Effects: MemoryEffects::Effect{}</p><h4 id=operands-1>Operands:&nbsp;<a class=headline-hash href=#operands-1>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>sizes</code></td><td>index</td></tr></tbody></table><h4 id=results-1>Results:&nbsp;<a class=headline-hash href=#results-1>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>result</code></td><td>tensor of any type values</td></tr></tbody></table><h3 id=sparse_tensorlex_insert-mlirsparse_tensorlexinsertop><code>sparse_tensor.lex_insert</code> (::mlir::sparse_tensor::LexInsertOp)&nbsp;<a class=headline-hash href=#sparse_tensorlex_insert-mlirsparse_tensorlexinsertop>¶</a></h3><p>Inserts a value into given sparse tensor in lexicograph index order</p><p>Syntax:</p><pre><code>operation ::= `sparse_tensor.lex_insert` $tensor `,` $indices `,` $value attr-dict `:` type($tensor) `,` type($indices) `,` type($value)
</code></pre><p>Inserts the given value at given indices into the underlying sparse
storage format of the given tensor with the given indices. This
operation can only be applied when a tensor materializes unintialized
with an <code>init</code> operation, the insertions occur in strict lexicographic
index order, and the final tensor is constructed with a <code>tensor</code>
operation that has the <code>hasInserts</code> attribute set.</p><p>Note that this operation is &ldquo;impure&rdquo; in the sense that its behavior
is solely defined by side-effects and not SSA values. The semantics
may be refined over time as our sparse abstractions evolve.</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir>sparse_tensor<span class=p>.</span>lex_insert <span class=nv>%tensor</span><span class=p>,</span> <span class=nv>%indices</span><span class=p>,</span> <span class=nv>%val</span>
  <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>1024x1024x</span><span class=k>f64</span><span class=p>,</span> <span class=nv>#CSR</span><span class=p>&gt;,</span> <span class=kt>memref</span><span class=p>&lt;</span><span class=m>?x</span><span class=k>index</span><span class=p>&gt;,</span> <span class=k>f64</span>
</code></pre></div><h4 id=operands-2>Operands:&nbsp;<a class=headline-hash href=#operands-2>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>tensor</code></td><td>tensor of any type values</td></tr><tr><td style=text-align:center><code>indices</code></td><td>1D memref of index values</td></tr><tr><td style=text-align:center><code>value</code></td><td>any type</td></tr></tbody></table><h3 id=sparse_tensorload-mlirsparse_tensorloadop><code>sparse_tensor.load</code> (::mlir::sparse_tensor::LoadOp)&nbsp;<a class=headline-hash href=#sparse_tensorload-mlirsparse_tensorloadop>¶</a></h3><p>Rematerializes tensor from underlying sparse storage format</p><p>Syntax:</p><pre><code>operation ::= `sparse_tensor.load` $tensor (`hasInserts` $hasInserts^)? attr-dict `:` type($tensor)
</code></pre><p>Rematerializes a tensor from the underlying sparse storage format of the
given tensor. This is similar to the <code>bufferization.to_tensor</code> operation
in the sense that it provides a bridge between a bufferized world view
and a tensor world view. Unlike the <code>bufferization.to_tensor</code> operation,
however, this sparse operation is used only temporarily to maintain a
correctly typed intermediate representation during progressive
bufferization.</p><p>The <code>hasInserts</code> attribute denote whether insertions to the underlying
sparse storage format may have occurred, in which case the underlying
sparse storage format needs to be finalized. Otherwise, the operation
simply folds away.</p><p>Note that this operation is &ldquo;impure&rdquo; in the sense that its behavior
is solely defined by side-effects and not SSA values. The semantics
may be refined over time as our sparse abstractions evolve.</p><p>Example:</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=nv>%1</span> <span class=p>=</span> sparse_tensor<span class=p>.</span>load <span class=nv>%0</span> <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>8x</span><span class=k>f64</span><span class=p>,</span> <span class=nv>#SV</span><span class=p>&gt;</span>
</code></pre></div><p>Traits: SameOperandsAndResultType</p><h4 id=attributes>Attributes:&nbsp;<a class=headline-hash href=#attributes>¶</a></h4><table><thead><tr><th style=text-align:center>Attribute</th><th style=text-align:center>MLIR Type</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>hasInserts</code></td><td style=text-align:center>::mlir::UnitAttr</td><td>unit attribute</td></tr></tbody></table><h4 id=operands-3>Operands:&nbsp;<a class=headline-hash href=#operands-3>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>tensor</code></td><td>tensor of any type values</td></tr></tbody></table><h4 id=results-2>Results:&nbsp;<a class=headline-hash href=#results-2>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>result</code></td><td>tensor of any type values</td></tr></tbody></table><h3 id=sparse_tensornew-mlirsparse_tensornewop><code>sparse_tensor.new</code> (::mlir::sparse_tensor::NewOp)&nbsp;<a class=headline-hash href=#sparse_tensornew-mlirsparse_tensornewop>¶</a></h3><p>Materializes a new sparse tensor from given source</p><p>Syntax:</p><pre><code>operation ::= `sparse_tensor.new` $source attr-dict `:` type($source) `to` type($result)
</code></pre><p>Materializes a sparse tensor with contents taken from an opaque pointer
provided by <code>source</code>. For targets that have access to a file system,
for example, this pointer may be a filename (or file) of a sparse
tensor in a particular external storage format. The form of the operation
is kept deliberately very general to allow for alternative implementations
in the future, such as pointers to buffers or runnable initialization
code. The operation is provided as an anchor that materializes a properly
typed sparse tensor with inital contents into a computation.</p><p>Example:</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir>sparse_tensor<span class=p>.</span>new <span class=nv>%source</span> <span class=p>:</span> <span class=p>!</span>Source to <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>1024x1024x</span><span class=k>f64</span><span class=p>,</span> <span class=nv>#CSR</span><span class=p>&gt;</span>
</code></pre></div><p>Interfaces: NoSideEffect (MemoryEffectOpInterface)</p><p>Effects: MemoryEffects::Effect{}</p><h4 id=operands-4>Operands:&nbsp;<a class=headline-hash href=#operands-4>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>source</code></td><td>any type</td></tr></tbody></table><h4 id=results-3>Results:&nbsp;<a class=headline-hash href=#results-3>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>result</code></td><td>tensor of any type values</td></tr></tbody></table><h3 id=sparse_tensorrelease-mlirsparse_tensorreleaseop><code>sparse_tensor.release</code> (::mlir::sparse_tensor::ReleaseOp)&nbsp;<a class=headline-hash href=#sparse_tensorrelease-mlirsparse_tensorreleaseop>¶</a></h3><p>Releases underlying sparse storage format of given tensor</p><p>Syntax:</p><pre><code>operation ::= `sparse_tensor.release` $tensor attr-dict `:` type($tensor)
</code></pre><p>Releases the underlying sparse storage format for a tensor that
materialized earlier through a <code>new</code> operator, <code>init</code> operator, or a
<code>convert</code> operator with an annotated tensor type as destination (unless
that convert is folded away since the source and destination types were
identical). This operation should only be called once for any materialized
tensor. Also, after this operation, any subsequent <code>memref</code> querying
operation on the tensor returns undefined results.</p><p>Note that this operation is &ldquo;impure&rdquo; in the sense that its behavior
is solely defined by side-effects and not SSA values. The semantics
may be refined over time as our sparse abstractions evolve.</p><p>Example:</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir>sparse_tensor<span class=p>.</span>release <span class=nv>%tensor</span> <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>1024x1024x</span><span class=k>f64</span><span class=p>,</span> <span class=nv>#CSR</span><span class=p>&gt;</span>
</code></pre></div><h4 id=operands-5>Operands:&nbsp;<a class=headline-hash href=#operands-5>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>tensor</code></td><td>tensor of any type values</td></tr></tbody></table><h3 id=sparse_tensorindices-mlirsparse_tensortoindicesop><code>sparse_tensor.indices</code> (::mlir::sparse_tensor::ToIndicesOp)&nbsp;<a class=headline-hash href=#sparse_tensorindices-mlirsparse_tensortoindicesop>¶</a></h3><p>Extracts indices array at given dimension from a tensor</p><p>Syntax:</p><pre><code>operation ::= `sparse_tensor.indices` $tensor `,` $dim attr-dict `:` type($tensor) `to` type($result)
</code></pre><p>Returns the indices array of the sparse storage format at the
given dimension for the given sparse tensor. This is similar to the
<code>bufferization.to_memref</code> operation in the sense that it provides a bridge
between a tensor world view and a bufferized world view. Unlike the
<code>bufferization.to_memref</code> operation, however, this sparse operation actually
lowers into a call into a support library to obtain access to the
indices array.</p><p>Example:</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=nv>%1</span> <span class=p>=</span> sparse_tensor<span class=p>.</span>indices <span class=nv>%0</span><span class=p>,</span> <span class=nv>%c1</span>
   <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>64x64x</span><span class=k>f64</span><span class=p>,</span> <span class=nv>#CSR</span><span class=p>&gt;</span> to <span class=kt>memref</span><span class=p>&lt;</span><span class=m>?x</span><span class=k>index</span><span class=p>&gt;</span>
</code></pre></div><p>Interfaces: NoSideEffect (MemoryEffectOpInterface)</p><p>Effects: MemoryEffects::Effect{}</p><h4 id=operands-6>Operands:&nbsp;<a class=headline-hash href=#operands-6>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>tensor</code></td><td>tensor of any type values</td></tr><tr><td style=text-align:center><code>dim</code></td><td>index</td></tr></tbody></table><h4 id=results-4>Results:&nbsp;<a class=headline-hash href=#results-4>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>result</code></td><td>strided memref of any type values of rank 1</td></tr></tbody></table><h3 id=sparse_tensorpointers-mlirsparse_tensortopointersop><code>sparse_tensor.pointers</code> (::mlir::sparse_tensor::ToPointersOp)&nbsp;<a class=headline-hash href=#sparse_tensorpointers-mlirsparse_tensortopointersop>¶</a></h3><p>Extracts pointers array at given dimension from a tensor</p><p>Syntax:</p><pre><code>operation ::= `sparse_tensor.pointers` $tensor `,` $dim attr-dict `:` type($tensor) `to` type($result)
</code></pre><p>Returns the pointers array of the sparse storage format at the
given dimension for the given sparse tensor. This is similar to the
<code>bufferization.to_memref</code> operation in the sense that it provides a bridge
between a tensor world view and a bufferized world view. Unlike the
<code>bufferization.to_memref</code> operation, however, this sparse operation actually
lowers into a call into a support library to obtain access to the
pointers array.</p><p>Example:</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=nv>%1</span> <span class=p>=</span> sparse_tensor<span class=p>.</span>pointers <span class=nv>%0</span><span class=p>,</span> <span class=nv>%c1</span>
   <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>64x64x</span><span class=k>f64</span><span class=p>,</span> <span class=nv>#CSR</span><span class=p>&gt;</span> to <span class=kt>memref</span><span class=p>&lt;</span><span class=m>?x</span><span class=k>index</span><span class=p>&gt;</span>
</code></pre></div><p>Interfaces: NoSideEffect (MemoryEffectOpInterface)</p><p>Effects: MemoryEffects::Effect{}</p><h4 id=operands-7>Operands:&nbsp;<a class=headline-hash href=#operands-7>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>tensor</code></td><td>tensor of any type values</td></tr><tr><td style=text-align:center><code>dim</code></td><td>index</td></tr></tbody></table><h4 id=results-5>Results:&nbsp;<a class=headline-hash href=#results-5>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>result</code></td><td>strided memref of any type values of rank 1</td></tr></tbody></table><h3 id=sparse_tensorvalues-mlirsparse_tensortovaluesop><code>sparse_tensor.values</code> (::mlir::sparse_tensor::ToValuesOp)&nbsp;<a class=headline-hash href=#sparse_tensorvalues-mlirsparse_tensortovaluesop>¶</a></h3><p>Extracts numerical values array from a tensor</p><p>Syntax:</p><pre><code>operation ::= `sparse_tensor.values` $tensor attr-dict `:` type($tensor) `to` type($result)
</code></pre><p>Returns the values array of the sparse storage format for the given
sparse tensor, independent of the actual dimension. This is similar to
the <code>bufferization.to_memref</code> operation in the sense that it provides a bridge
between a tensor world view and a bufferized world view. Unlike the
<code>bufferization.to_memref</code> operation, however, this sparse operation actually
lowers into a call into a support library to obtain access to the
values array.</p><p>Example:</p><div class=highlight><pre class=chroma><code class=language-mlir data-lang=mlir><span class=nv>%1</span> <span class=p>=</span> sparse_tensor<span class=p>.</span>values <span class=nv>%0</span> <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>64x64x</span><span class=k>f64</span><span class=p>,</span> <span class=nv>#CSR</span><span class=p>&gt;</span> to <span class=kt>memref</span><span class=p>&lt;</span><span class=m>?x</span><span class=k>f64</span><span class=p>&gt;</span>
</code></pre></div><p>Interfaces: NoSideEffect (MemoryEffectOpInterface)</p><p>Effects: MemoryEffects::Effect{}</p><h4 id=operands-8>Operands:&nbsp;<a class=headline-hash href=#operands-8>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>tensor</code></td><td>tensor of any type values</td></tr></tbody></table><h4 id=results-6>Results:&nbsp;<a class=headline-hash href=#results-6>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>result</code></td><td>strided memref of any type values of rank 1</td></tr></tbody></table><div class=edit-meta><br></div><nav class=pagination><a class="nav nav-prev" href=/docs/Dialects/ShapeDialect/ title="'shape' Dialect"><i class="fas fa-arrow-left" aria-hidden=true></i>Prev - 'shape' Dialect</a>
<a class="nav nav-next" href=/docs/Dialects/SPIR-V/ title="'spv' Dialect">Next - 'spv' Dialect <i class="fas fa-arrow-right" aria-hidden=true></i></a></nav><footer><p class=powered>Powered by <a href=https://gohugo.io>Hugo</a>. Theme by <a href=https://themes.gohugo.io/hugo-theme-techdoc/>TechDoc</a>. Designed by <a href=https://github.com/thingsym/hugo-theme-techdoc>Thingsym</a>.</p></footer></main><div class=sidebar><nav class=slide-menu><ul><li><a href=https://mlir.llvm.org/>Home</a></li><li><a href=/pubs/>MLIR Related Publications</a></li><li><a href=/talks/>Talks</a></li><li><a href=/users/>Users of MLIR</a></li><li class=has-sub-menu><a href=/getting_started/>Getting Started<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=/getting_started/Debugging/>Debugging</a></li><li><a href=/getting_started/Faq/>FAQ</a></li><li><a href=/getting_started/Contributing/>How to Contribute</a></li><li><a href=/getting_started/DeveloperGuide/>Developer Guide</a></li><li><a href=/getting_started/openprojects/>Open Projects</a></li><li><a href=/getting_started/Glossary/>Glossary</a></li><li><a href=/getting_started/TestingGuide/>Testing Guide</a></li></ul></li><li class="parent has-sub-menu"><a href=/docs/>Code Documentation<span class="mark opened">-</span></a><ul class=sub-menu><li class=has-sub-menu><a href=/docs/Bindings/>Bindings<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=/docs/Bindings/Python/>MLIR Python Bindings</a></li></ul></li><li class=has-sub-menu><a href=/docs/Tools/>Tools<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=/docs/Tools/MLIRLSP/>MLIR : Language Server Protocol</a></li><li><a href=/docs/Tools/mlir-reduce/>MLIR Reduce</a></li></ul></li><li><a href=/docs/BufferizationPasses/></a></li><li><a href=/docs/BufferDeallocationInternals/>Buffer Deallocation - Internals</a></li><li><a href=/docs/Bufferization/>Bufferization</a></li><li><a href=/docs/DataLayout/>Data Layout Modeling</a></li><li><a href=/docs/DebugActions/>Debug Actions</a></li><li><a href=/docs/Diagnostics/>Diagnostic Infrastructure</a></li><li><a href=/docs/DialectConversion/>Dialect Conversion</a></li><li class="parent has-sub-menu"><a href=/docs/Dialects/>Dialects<span class="mark opened">-</span></a><ul class=sub-menu><li><a href=/docs/Dialects/DLTIDialect/></a></li><li><a href=/docs/Dialects/OpenACCDialect/>'acc' Dialect</a></li><li><a href=/docs/Dialects/Affine/>'affine' Dialect</a></li><li><a href=/docs/Dialects/AMX/>'amx' Dialect</a></li><li><a href=/docs/Dialects/ArithmeticOps/>'arith' Dialect</a></li><li><a href=/docs/Dialects/ArmNeon/>'arm_neon' Dialect</a></li><li><a href=/docs/Dialects/ArmSVE/>'arm_sve' Dialect</a></li><li><a href=/docs/Dialects/AsyncDialect/>'async' Dialect</a></li><li><a href=/docs/Dialects/BufferizationOps/>'bufferization' Dialect</a></li><li><a href=/docs/Dialects/ComplexOps/>'complex' Dialect</a></li><li><a href=/docs/Dialects/EmitC/>'emitc' Dialect</a></li><li><a href=/docs/Dialects/GPU/>'gpu' Dialect</a></li><li class=has-sub-menu><a href=/docs/Dialects/Linalg/>'linalg' Dialect<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=/docs/Dialects/Linalg/OpDSL/>Linalg OpDSL</a></li></ul></li><li><a href=/docs/Dialects/LLVM/>'llvm' Dialect</a></li><li><a href=/docs/Dialects/MathOps/>'math' Dialect</a></li><li><a href=/docs/Dialects/MemRef/>'memref' Dialect</a></li><li><a href=/docs/Dialects/NVVMDialect/>'nvvm' Dialect</a></li><li><a href=/docs/Dialects/OpenMPDialect/>'omp' Dialect</a></li><li><a href=/docs/Dialects/PDLOps/>'pdl' Dialect</a></li><li><a href=/docs/Dialects/PDLInterpOps/>'pdl_interp' Dialect</a></li><li><a href=/docs/Dialects/QuantDialect/>'quant' Dialect</a></li><li><a href=/docs/Dialects/ROCDLDialect/>'rocdl' Dialect</a></li><li><a href=/docs/Dialects/SCFDialect/>'scf' Dialect</a></li><li><a href=/docs/Dialects/ShapeDialect/>'shape' Dialect</a></li><li class=active><a href=/docs/Dialects/SparseTensorOps/>'sparse_tensor' Dialect</a></li><li><a href=/docs/Dialects/SPIR-V/>'spv' Dialect</a></li><li><a href=/docs/Dialects/Standard/>'std' Dialect</a></li><li><a href=/docs/Dialects/TensorOps/>'tensor' Dialect</a></li><li><a href=/docs/Dialects/Vector/>'vector' Dialect</a></li><li><a href=/docs/Dialects/X86Vector/>'x86vector' Dialect</a></li><li><a href=/docs/Dialects/Builtin/>Builtin Dialect</a></li><li><a href=/docs/Dialects/TOSA/>Tensor Operator Set Architecture (TOSA) Dialect</a></li></ul></li><li><a href=/docs/Interfaces/>Interfaces</a></li><li><a href=/docs/TargetLLVMIR/>LLVM IR Target</a></li><li><a href=/docs/CAPI/>MLIR C API</a></li><li><a href=/docs/LangRef/>MLIR Language Reference</a></li><li><a href=/docs/Canonicalization/>Operation Canonicalization</a></li><li><a href=/docs/OpDefinitions/>Operation Definition Specification (ODS)</a></li><li><a href=/docs/PassManagement/>Pass Infrastructure</a></li><li><a href=/docs/Passes/>Passes</a></li><li><a href=/docs/PatternRewriter/>Pattern Rewriting : Generic DAG-to-DAG Rewriting</a></li><li><a href=/docs/Quantization/>Quantization</a></li><li class=has-sub-menu><a href=/docs/Rationale/>Rationale<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=/docs/Rationale/RationaleGenericDAGRewriter/>Generic DAG Rewriter Infrastructure Rationale</a></li><li><a href=/docs/Rationale/RationaleLinalgDialect/>Linalg Dialect Rationale: The Case For Compiler-Friendly Custom Operations</a></li><li><a href=/docs/Rationale/Rationale/>MLIR Rationale</a></li><li><a href=/docs/Rationale/MLIRForGraphAlgorithms/>MLIR: Incremental Application to Graph Algorithms in ML Frameworks</a></li><li><a href=/docs/Rationale/RationaleSimplifiedPolyhedralForm/>MLIR: The case for a simplified polyhedral form</a></li><li><a href=/docs/Rationale/UsageOfConst/>Usage of 'const' in MLIR, for core IR types</a></li></ul></li><li><a href=/docs/ShapeInference/>Shape Inference</a></li><li><a href=/docs/SPIRVToLLVMDialectConversion/>SPIR-V Dialect to LLVM Dialect conversion manual</a></li><li><a href=/docs/SymbolsAndSymbolTables/>Symbols and Symbol Tables</a></li><li><a href=/docs/DeclarativeRewrites/>Table-driven Declarative Rewrite Rule (DRR)</a></li><li><a href=/docs/Traits/>Traits</a></li><li class=has-sub-menu><a href=/docs/Tutorials/>Tutorials<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=/docs/Tutorials/CreatingADialect/>Creating a Dialect</a></li><li><a href=/docs/Tutorials/DefiningAttributesAndTypes/>Defining Dialect Attributes and Types</a></li><li><a href=/docs/Tutorials/QuickstartRewrites/>Quickstart tutorial to adding MLIR graph rewrite</a></li><li class=has-sub-menu><a href=/docs/Tutorials/Toy/>Toy Tutorial<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=/docs/Tutorials/Toy/Ch-1/>Chapter 1: Toy Language and AST</a></li><li><a href=/docs/Tutorials/Toy/Ch-2/>Chapter 2: Emitting Basic MLIR</a></li><li><a href=/docs/Tutorials/Toy/Ch-3/>Chapter 3: High-level Language-Specific Analysis and Transformation</a></li><li><a href=/docs/Tutorials/Toy/Ch-4/>Chapter 4: Enabling Generic Transformation with Interfaces</a></li><li><a href=/docs/Tutorials/Toy/Ch-5/>Chapter 5: Partial Lowering to Lower-Level Dialects for Optimization</a></li><li><a href=/docs/Tutorials/Toy/Ch-6/>Chapter 6: Lowering to LLVM and CodeGeneration</a></li><li><a href=/docs/Tutorials/Toy/Ch-7/>Chapter 7: Adding a Composite Type to Toy</a></li></ul></li><li><a href=/docs/Tutorials/UnderstandingTheIRStructure/>Understanding the IR Structure</a></li><li><a href=/docs/Tutorials/DataFlowAnalysis/>Writing DataFlow Analyses in MLIR</a></li></ul></li></ul></li></ul></nav><div class=sidebar-footer></div></div></div><a href=# id=backtothetop-fixed class=backtothetop data-backtothetop-duration=600 data-backtothetop-easing=easeOutQuart data-backtothetop-fixed-fadein=1000 data-backtothetop-fixed-fadeout=1000 data-backtothetop-fixed-bottom=10 data-backtothetop-fixed-right=20><span class="fa-layers fa-fw"><i class="fas fa-circle"></i><i class="fas fa-arrow-circle-up"></i></span></a></div></body></html>